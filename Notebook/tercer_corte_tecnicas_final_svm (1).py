# -*- coding: utf-8 -*-
"""TERCER_CORTE_TECNICAS_FINAL_SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Cp_ALWoK_HqkS5mbnxtfvHSiB4LBrIt
"""

!pip install deap

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.base import clone
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

#Librerias de el algoritmo genetico
import random
from deap import algorithms
from deap import base
from deap import creator
from deap import tools
from sklearn.model_selection import KFold

def proporcion_variables(dataset,columna):
  unicos = list(dataset[columna].unique())
  proporcion = []
  for categoria in unicos:
    tamaño = dataset.loc[dataset[columna] == categoria].shape[0]
    proporcion.append(tamaño/dataset.shape[0])

  print(f"Para la columna {columna} las proporciones son:")
  for categoria,proporcion in zip(unicos,proporcion):
    print(f"{categoria}: {proporcion}")
  print("\n")

def calculate_fitness(model, x, y):
    cv_set = np.repeat(-1.0, x.shape[0])
    skf = StratifiedKFold(n_splits=5)
    for train_index, test_index in skf.split(x, y):
        x_train, x_test = x[train_index], x[test_index]
        y_train, y_test = y[train_index], y[test_index]
        if x_train.shape[0] != y_train.shape[0]:
            raise Exception()
        model.fit(x_train, y_train)
        predicted_y = model.predict(x_test)
        cv_set[test_index] = predicted_y
    return accuracy_score(y,cv_set)

def evaluate(individual):
    np_ind = np.asarray(individual)
    if np.sum(np_ind) == 0:
        fitness = 0.0
    else:
        feature_idx = np.where(np_ind == 1)[0]
        fitness = calculate_fitness(
            model, X[:, feature_idx], y
        )
        if verbose:
            print("Individuo: {}  Fitness Score: {} ".format(individual, fitness))

    return (fitness,)

# Función para generar salidas con estadisticas de cada generacion
def build_stats(gen, pop, fits):
    record = {}
    length = len(pop)
    mean = sum(fits) / length
    sum2 = sum(x * x for x in fits)
    std = abs(sum2 / length - mean ** 2) ** 0.5

    record['gen'] = gen + 1
    record['min'] = min(fits)
    record['max'] = max(fits)
    record['avg'] = mean
    record['std'] = std

    print("  Min {}  Max {}  Avg {}  Std {}".format(min(fits), max(fits), mean, std))

    return record

def seleccion_variables(X,y, n_features, verbose, model):
  creator.create("FeatureSelect", base.Fitness, weights=(1.0,))
  creator.create("Individual", list, fitness=creator.FeatureSelect)
  toolbox = base.Toolbox()

  toolbox.register("attr_bool", random.randint, 0, 1)
  toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, n_features)
  toolbox.register("population", tools.initRepeat, list, toolbox.individual)

  toolbox.register("mate", tools.cxTwoPoint) # Crossover
  toolbox.register("mutate", tools.mutFlipBit, indpb=0.1) # Mutacion
  toolbox.register("select", tools.selTournament, tournsize=3) # Selecion
  toolbox.register("evaluate", evaluate) # Evaluacion

  N_POP = 100 # Tamaño de la población
  CXPB = 0.5 # Probabilidad de crossover
  MUTPB = 0.2 # Probabilidad de mutación
  NGEN = 10 # Cantidad de generaciones

  print(
      "Tamaño población: {}\nProbabilidad de crossover: {}\nProbabilida de mutación: {}\nGeneraciones totales: {}".format(
          N_POP, CXPB, MUTPB, NGEN
      )
  )
  # Inicializamos a la poblacion
  pop = toolbox.population(N_POP)

  print("Evaluamos a los individuos inicializados.......")
  fitnesses = list(map(toolbox.evaluate, pop))

  # Asignamos a los inviduos el score del paso anterior
  for ind, fit in zip(pop, fitnesses):
      ind.fitness.values = fit

  fitness_in_generation = {} # Variable auxiliar para generar el reporte
  stats_records = [] # Variable auxiliar para generar el reporte

  print("-- GENERACIÓN 0 --")
  stats_records.append(build_stats(-1, pop, fitnesses[0]))

  for g in range(NGEN):
      print("-- GENERACIÓN {} --".format(g + 1))
      # Seleccionamos a la siguiente generacion de individuos
      offspring = toolbox.select(pop, len(pop))

      # Clonamos a los invidiuos seleccionados
      offspring = list(map(toolbox.clone, offspring))

      # Aplicamos crossover y mutacion a los inviduos seleccionados
      for child1, child2 in zip(offspring[::2], offspring[1::2]):
          if random.random() < CXPB:
              toolbox.mate(child1, child2)
              del child1.fitness.values
              del child2.fitness.values

      for mutant in offspring:
          if random.random() < MUTPB:
              toolbox.mutate(mutant)
              del mutant.fitness.values


      # Evaluamos a los individuos con una fitness invalida
      weak_ind = [ind for ind in offspring if not ind.fitness.valid]
      fitnesses = list(map(toolbox.evaluate, weak_ind))
      for ind, fit in zip(weak_ind, fitnesses):
          ind.fitness.values = fit
      print("Individuos evaluados: {}".format(len(weak_ind)))

      # Reemplazamos a la poblacion completamente por los nuevos descendientes
      pop[:] = offspring

      # Mostramos las salidas de la estadisticas de la generacion actual
      fits = [ind.fitness.values[0] for ind in pop]

      stats_records.append(build_stats(g, pop, fits))

  best_solution = tools.selBest(pop, 1)[0]
  print(
        "El mejor individuo es: \n{}, con un Accuracy de {}".format(best_solution, best_solution.fitness.values)
  )
  index_list = []
  for i in range(len(best_solution)):
    if best_solution[i] == 1:
      index_list.append(i)
  return index_list

data = pd.read_csv('/content/health_data (1).csv')

"""# Analisis exploratorio de datos"""

data

numericas = ['Edad', 'Altura', 'Peso', 'Índice de masa corporal',
       'Enfermedad cardiovascular', 'Diabetes', 'Asma', 'Cáncer', 'Obesidad',
       'Depresión/Ansiedad']
categoricas = ['Género','Estado civil','¿Fuma actualmente?','¿Fumó en el pasado?','¿Consume alcohol frecuentemente?','¿Tiene una dieta equilibrada?',
    '¿Consume frutas y verduras diariamente?','¿Duerme al menos 7 horas por noche?','¿Experimenta estrés con frecuencia?','¿Tiene antecedentes de hipertensión en la familia?',
    '¿Tiene antecedentes de diabetes en la familia?','¿Tiene antecedentes de cáncer en la familia?','¿Tiene antecedentes de enfermedades cardiovasculares en la familia?','¿Tiene antecedentes de problemas de tiroides en la familia?',
    '¿Toma medicamentos regularmente?','¿Tiene antecedentes de obesidad en la familia?','¿Tiene antecedentes de asma?','¿Padece de alguna alergia?',
    '¿Ha tenido infecciones respiratorias frecuentes en el último año?','¿Tiene dificultades para respirar durante el ejercicio?','¿Toma suplementos vitamínicos?',
    '¿Ha experimentado pérdida de peso no intencionada?','¿Ha tenido dolor en el pecho recientemente?','¿Experimenta dolor en las articulaciones?','¿Tiene problemas para conciliar el sueño?',
    '¿Ha tenido tos persistente en los últimos 3 meses?','¿Ha experimentado depresión o ansiedad?','¿Consume alimentos procesados frecuentemente?','¿Padece de insomnio?',
    'Frecuencia de chequeos médicos','¿Tiene problemas digestivos frecuentes?','¿Ha tenido infecciones frecuentes?',
    '¿Sufre de problemas de visión?','¿Tiene problemas de audición?','¿Ha sufrido de fracturas óseas en el pasado?'
]
ordinal_columns = {
    'Nivel de actividad física': ['Sedentario', 'Moderado', 'Activo'],
    'Frecuencia de consumo de comida rápida': ['Nunca', 'Ocasional', 'Frecuente'],
    'Frecuencia de ejercicio físico semanal': ['0', '1-2', '3-4', '5 o más'],
    'Nivel de colesterol': ['Normal', 'Alto'],
    'Nivel de triglicéridos': ['Normal', 'Alto'],
    'Nivel de glucosa en sangre': ['Normal', 'Alto'],
    'Presión arterial': ['Normal', 'Alta'],
    'Consumo de sal en la dieta': ['Bajo', 'Medio', 'Alto'],
    'Nivel de azúcar en la dieta': ['Bajo', 'Medio', 'Alto'],
    'Nivel de estrés laboral o académico': ['Bajo', 'Medio', 'Alto'],
    'Nivel de satisfacción con la vida': ['Bajo', 'Medio', 'Alto']
}

data.describe()

n = len(numericas)
columnas = 3
filas = 4

fig, axes = plt.subplots(filas, columnas, figsize=(15,12))
axes = axes.flatten()

for i, columna in enumerate(numericas):
    axes[i].hist(data[columna])
    axes[i].set_title(f"Histograma {columna}")
    axes[i].set_xlabel("Valor")
    axes[i].set_ylabel("Frecuencia")

# Eliminar ejes sobrantes
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""**Inconsistencias y errores encontrados**


*   **Edad**: Encontramos varias inconsistencias, como edades negativas, lo cual no es posible. Esto podría deberse a errores en la recolección de datos. Este tipo de registros puede afectar negativamente el análisis, por lo que lo más recomendable es eliminarlos
* **Altura**: La altura mínima registrada es de 125 cm, lo cual tiene sentido si consideramos que el conjunto de datos incluye niños. Sin embargo, también se presentan valores inconsistentes. Por ejemplo, al aplicar la siguiente línea de código: **data.loc[(data['Altura'] < 161) & (data['Edad'] < 14)]**  encontramos una niña de 5 años con una altura de 156 cm, lo cual resulta poco creíble.
* **Peso**: Parece que en peso hay muchas inconcistencias que pueden afectar el analisis, por ejemplo hay un individuo con 18kg como peso y una altura de 184cm.
* **Indice de masa corporal**:  A primera vista, los valores de IMC no presentan inconsistencias evidentes. Sin embargo, al recalcular el IMC con la fórmula **IMC = PESO(KG)/ALTURA(M)^2**. se observa que muchos valores no coinciden con los registrados en la columna correspondiente. Por lo tanto, se procederá a corregirlos para evitar errores en el análisis
* **Enfermedad cardiovascular**, Diabetes, Asma, Cáncer, Obesidad, Depresión/Ansiedad(Variables objetivo): Estas variables representan la probabilidad de desarrollar alguna de las enfermedades mencionadas. Teniendo esto en cuenta, es necesario limpiar los datos, ya que se encontraron valores negativos y/o superiores a 1, lo cual no es válido.

**Distribución de los datos**

* **IMC, Edad, Altura y Peso**: Según nuestros histogramas, estos datos siguen una distribución aproximadamente normal.

* **Enfermedad cardiovascular, Diabetes, Asma, Cáncer, Obesidad, Depresión/Ansiedad (variables objetivo)**: Observamos que estas variables presentan una distribución bimodal, lo cual sugiere la presencia de un efecto categórico en los datos. Por lo tanto, consideramos apropiado utilizar un umbral para clasificar a los pacientes como enfermos o no enfermos.
"""

data.loc[((data['Estado civil']== "Divorciado") & (data["Edad"]<=18))]

data.loc[~((data['Estado civil']== "Divorciado") & (data["Edad"]<=18))]

for columna in categoricas:
  proporcion_variables(data,columna)

"""# Preprocesamiento de datos"""

data['Índice de masa corporal'] = data["Peso"]/(data["Altura"]/100)**2

"""**Eliminación de inconcistencias**"""

data = data.loc[data['Edad']>14]
data = data.loc[~((data['Estado civil']== "Divorciado") & (data["Edad"]<=18))]
data = data.loc[(data['Índice de masa corporal'] > 10) & (data['Índice de masa corporal'] < 60)]

data.describe()

"""**Transformacion de variables**"""

for columna in [ 'Enfermedad cardiovascular', 'Diabetes', 'Asma', 'Cáncer', 'Obesidad','Depresión/Ansiedad']:
  data[columna] = data[columna].apply(lambda x: 0 if x < 0.5 else 1)
  categoricas.append(columna)
  numericas.remove(columna)

"""**Hallazgos encontrados**

En las siguientes líneas de código se puede observar que los datos del modelo son prácticamente iguales tanto para los grupos que padecen obesidad como para los que no (lo mismo ocurre con las demás variables objetivo). Esto se debe a que las variables numéricas presentan una desviación estándar, media y otros estadísticos similares —o muy parecidos— entre ambos grupos. Como consecuencia, resulta muy difícil que alguno de los modelos propuestos logre clasificar correctamente dichas variables objetivo.

Esta limitación se hace especialmente evidente más adelante, cuando probamos el algoritmo genético en combinación tanto con el árbol de clasificación como con la regresión logística. A pesar de contar con la mejor selección de variables, el accuracy máximo obtenido tiende a ser de aproximadamente 0.50, lo que indica que el modelo no es capaz de clasificar correctamente, por la razón previamente mencionada.
"""

data.loc[data['Obesidad']==1].describe()

data.loc[data['Obesidad'] == 0].describe()

for col, categories in ordinal_columns.items():
    encoder = OrdinalEncoder(categories=[categories])
    data[col] = encoder.fit_transform(data[[col]])

encoder = LabelEncoder()
for columna in categoricas:
  data[columna] = encoder.fit_transform(data[columna])

#Normalizacion de datos
scaler = StandardScaler()
data[numericas] = scaler.fit_transform(data[numericas])

x = data.iloc[:,:50]
y = data.iloc[:,51:56]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

"""# Seleccion de variables"""

verbose = 0
n_features = 49
model = tree.DecisionTreeClassifier()

X = data.iloc[:, :49].to_numpy()
y = data.iloc[:, 50].to_numpy()

"""Algoritmo genetico con un arbol de decisión"""

columnas_seleccionadas = []
for i in range(50,56):
  print("=================================================================")
  y = data.iloc[:, i].to_numpy()
  lista = seleccion_variables(X,y, n_features, verbose, model)
  columnas_seleccionadas.append(lista)
  print(f"variables seleccionadas para la columna {data.columns[i]}")
  print(data.columns[lista])
  print('\n')

"""Algoritmo genetico con el modelo de regresion logistica"""

rl_columns = []
model = LogisticRegression(solver='liblinear', class_weight='balanced')
for i in range(50,56):
  print("=================================================================")
  y = data.iloc[:, i].to_numpy()
  lista = seleccion_variables(X,y, n_features, verbose, model)
  rl_columns.append(lista)
  print(f"variables seleccionadas para la columna {data.columns[i]}")
  print(data.columns[lista])
  print('\n')

data_cardiovascular = pd.DataFrame()
data_diabetes = pd.DataFrame()
data_asma = pd.DataFrame()
data_cancer = pd.DataFrame()
data_obesidad = pd.DataFrame()
data_depresion_ansiedad = pd.DataFrame()

lista_datasets = []

lista_datasets.append(data_cardiovascular)
lista_datasets.append(data_diabetes)
lista_datasets.append(data_asma)
lista_datasets.append(data_cancer)
lista_datasets.append(data_obesidad)
lista_datasets.append(data_depresion_ansiedad)

index_objetivo = 50
for j in range(len(lista_datasets)):
    df = data.iloc[:, columnas_seleccionadas[j]]
    df = pd.concat([df, data.iloc[:, index_objetivo]], axis=1)
    lista_datasets[j] = df  # Actualiza el DataFrame en la lista
    index_objetivo += 1

"""**DECISION TREE**

TIEMPO DE DEMORA: 55 segundos
"""

# index_objetivo = 50
# for df in lista_datasets:
#   print(f"------------{data.columns[index_objetivo]}----------")
#   index_objetivo +=1
#   tam = df.shape[1]
#   x = df.iloc[:,:tam-2]
#   y = df.iloc[:,tam-1]
#   X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#   base_clf = DecisionTreeClassifier()
#   param_grid = {
#       'criterion': ['gini', 'entropy'],
#       'max_depth': [3, 5, 10],
#       'min_samples_split': [2, 5, 10],
#       'min_samples_leaf': [1, 2, 4],
#   }

#   grid_search = GridSearchCV(estimator=base_clf, param_grid=param_grid, cv=5, scoring='f1')
#   grid_search.fit(X_train, y_train)
#   print(grid_search.best_params_)
#   y_pred = grid_search.predict(X_test)
#   print("Accuracy:", accuracy_score(y_test, y_pred))
#   print("Precision:", precision_score(y_test, y_pred))
#   print("Recall:", recall_score(y_test, y_pred))
#   print("F1-Score:", f1_score(y_test, y_pred))

"""**MODELO DE BAGGING**

TIEMPO DE DEMORA: 14 minutos
"""

# index_objetivo = 50
# for df in lista_datasets:
#   print(f"------------{data.columns[index_objetivo]}----------")
#   index_objetivo +=1
#   tam = df.shape[1]
#   x = df.iloc[:,:tam-2]
#   y = df.iloc[:,tam-1]
#   X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#   base_clf = DecisionTreeClassifier(criterion = "entropy",max_depth = 5,min_samples_split=2,class_weight="balanced")
#   # Modelo de bagging
#   bagging = BaggingClassifier(estimator=base_clf)
#   param_grid = {
#       'n_estimators': [10, 50, 100],
#       'max_samples': [0.5, 0.8, 1.0],
#       'estimator__max_depth': [3, 5, 10]
#   }
#   grid_search = GridSearchCV(estimator=bagging, param_grid=param_grid, cv=5, scoring='f1')
#   grid_search.fit(X_train,y_train)
#   print(grid_search.best_params_)
#   y_pred = grid_search.predict(X_test)
#   print("Accuracy:", accuracy_score(y_test, y_pred))
#   print("Precision:", precision_score(y_test, y_pred))
#   print("Recall:", recall_score(y_test, y_pred))
#   print("F1-Score:", f1_score(y_test, y_pred))

"""**MODELO DE BOOSTING**

TIEMPO DE DEMORA: 32 minutos
"""

# index_objetivo = 50
# for df in lista_datasets:
#     print(f"------------{data.columns[index_objetivo]}----------")
#     index_objetivo += 1
#     tam = df.shape[1]
#     x = df.iloc[:, :tam-2]
#     y = df.iloc[:, tam-1]

#     X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
#     base_clf = GradientBoostingClassifier()
#     param_grid = {
#         'n_estimators': [50, 100, 150],
#         'learning_rate': [0.01, 0.1, 0.2],
#         'max_depth': [3, 5, 7]
#     }
#     grid_search = GridSearchCV(estimator=base_clf, param_grid=param_grid, cv=5, scoring='f1')
#     grid_search.fit(X_train, y_train)

#     print(grid_search.best_params_)

#     y_pred = grid_search.predict(X_test)

#     print("Accuracy:", accuracy_score(y_test, y_pred))
#     print("Precision:", precision_score(y_test, y_pred))
#     print("Recall:", recall_score(y_test, y_pred))
#     print("F1-Score:", f1_score(y_test, y_pred))

index_objetivo = 50
for df in lista_datasets:
    print(f"------------{data.columns[index_objetivo]}----------")
    index_objetivo += 1

    tam = df.shape[1]
    x = df.iloc[:, :tam-2 nvb]
    y = df.iloc[:, tam-1]

    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)

    base_clf = SVC(probability=True)

    param_grid = {
        'C': [0.1, 1, 10],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto']
    }

    grid_search = GridSearchCV(estimator=base_clf, param_grid=param_grid, cv=5, scoring='f1')
    grid_search.fit(X_train, y_train)

    print("Mejores hiperparámetros:", grid_search.best_params_)

    y_pred = grid_search.predict(X_test)

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1-Score:", f1_score(y_test, y_pred))